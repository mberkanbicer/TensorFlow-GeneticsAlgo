{
  "cells": [
    {
      "metadata": {
        "_uuid": "c8fc6c3e84f282d0e6f664caf9848297b6c62a16"
      },
      "cell_type": "markdown",
      "source": ""
    },
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import os\nimport sys\nimport random\nfrom joblib import Parallel, delayed\nimport multiprocessing\nimport concurrent.futures\nimport time, random\nfrom numba import jit\nprint(\"done\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\n%matplotlib inline\n\n# import cv2\nfrom sklearn.model_selection import train_test_split\n\nfrom tqdm import tqdm_notebook, tnrange\nfrom itertools import chain\nfrom skimage.io import imread, imshow, concatenate_images\nfrom skimage.transform import resize\nfrom skimage.morphology import label\n\nfrom keras.models import Model, load_model\nfrom keras.layers import Input,Dropout,BatchNormalization,Activation,Add,UpSampling2D\nfrom keras.layers.core import Lambda\nfrom keras.layers.convolutional import Conv2D, Conv2DTranspose\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras import backend as K\n\nimport tensorflow as tf\n\nfrom keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img#,save_img",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Set some parameters\nimg_size_ori = 101\nimg_size_target = 128\nim_width = 101\nim_height = 101\nim_chan = 1\nbasicpath = '../input/'\npath_train = basicpath + 'train/'\npath_test = basicpath + 'test/'\n\npath_train_images = path_train + 'images/'\npath_train_masks = path_train + 'masks/'\npath_test_images = path_test + 'images/'\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1a64babef03b9a0dbc94387a1dad54971c3e028d",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Loading of training/testing ids and depths\n\ntrain_df = pd.read_csv(\"../input/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"../input/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]\nprint(len(depths_df))\nlen(train_df)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "80c3768717007fb5f087d3e01619f1a9f9a3beac",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "import cv2\ndef my_load_image(file):\n#img = cv2.imread('../input/train/images/9f3b8d0186.png')\n    img = cv2.imread(file)\n    img_to_yuv = cv2.cvtColor(img,cv2.COLOR_BGR2YUV)\n    img_to_yuv[:,:,0] = cv2.equalizeHist(img_to_yuv[:,:,0])\n    hist_equalization_result = cv2.cvtColor(img_to_yuv, cv2.COLOR_YUV2BGR)\n    #median = cv2.medianBlur(hist_equalization_result,5)\n    #kernel = np.ones((5,5),np.float32)/25\n    #dst = cv2.filter2D(hist_equalization_result,-1,kernel)\n    return hist_equalization_result\n\ntrain_df[\"images\"] = [np.array(load_img(\"../input/train/images/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f55103f7daad6f03ec874c643077fe686c31bee",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df[\"masks\"] = [np.array(load_img(\"../input/train/masks/{}.png\".format(idx), grayscale=True)) / 255 for idx in tqdm_notebook(train_df.index)]",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "010066dd50ef4fdfa7dabe2c946fd7491f9556fd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "train_df[\"coverage\"] = train_df.masks.map(np.sum) / pow(img_size_ori, 2)\n\ndef cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2ad5ac1576277fc54d768933c36efd1f9ff01acd",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9f34c4263989a3d95af1e5922c1b7d2126655610",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Plotting the depth distributionsÂ¶\n\nsns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "51981795f0dd6b8ca7abe4db367f48313b63811e",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Create train/validation split stratified by salt coverage\nids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.1, stratify=train_df.coverage_class, random_state=1337)\n\nx_train.shape",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b9d158f567c829c55139acc9e79a41761d911726",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\n\niou_thresholds = np.array([0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95])\n\ndef fitnes_lossfunction(imgs_true, imgs_pred):\n    num_images = len(imgs_true)\n    scores = np.zeros(num_images)\n    \n    for i in range(num_images):\n        if imgs_true[i].sum() == imgs_pred[i].sum() == 0:\n            scores[i] = 1\n        else:\n            scores[i] = (iou_thresholds <= iou(imgs_true[i], imgs_pred[i])).mean()\n            \n    return scores.mean()\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "072ab621d38cc93d26998f391357cb6efc791600",
        "scrolled": true,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Data augmentation\nx_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\ny_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\nprint(x_train.shape)\nprint(y_valid.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8253792cb5a6ad464d03073b282df45d2a975f40",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def small_ae():\n    input_img = Input(shape=(128, 128, 1))  # adapt this if using `channels_first` image data format\n    net = Conv2D( 32, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(input_img)\n    net = Conv2D(16, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(net)\n    net = Conv2D( 8, (5, 5), strides=4, padding='SAME',kernel_initializer='random_uniform')(net)\n    # decoder\n    # 2 x 2 x 8    ->  8 x 8 x 16\n    # 8 x 8 x 16   ->  16 x 16 x 32\n    # 16 x 16 x 32  ->  32 x 32 x 1\n    net = Conv2DTranspose( 16, (5, 5), strides=4, padding='SAME',kernel_initializer='random_uniform')(net)\n    net = Conv2DTranspose( 32, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform')(net)\n    decoded  = Conv2DTranspose( 1, (5, 5), strides=2, padding='SAME',kernel_initializer='random_uniform', activation='sigmoid')(net)\n    \n\n    autoencoder = Model(input_img, decoded )\n    autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy',metrics=['acc'])\n    return autoencoder\n\ndef small_unet():\n    inputs = Input((128, 128, 1))\n\n\n    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (inputs)\n    c1 = Conv2D(8, (3, 3), activation='relu', padding='same') (c1)\n    p1 = MaxPooling2D((2, 2)) (c1)\n\n    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (p1)\n    c2 = Conv2D(16, (3, 3), activation='relu', padding='same') (c2)\n    p2 = MaxPooling2D((2, 2)) (c2)\n\n    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (p2)\n    c3 = Conv2D(32, (3, 3), activation='relu', padding='same') (c3)\n    p3 = MaxPooling2D((2, 2)) (c3)\n\n    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (p3)\n    c4 = Conv2D(64, (3, 3), activation='relu', padding='same') (c4)\n    p4 = MaxPooling2D(pool_size=(2, 2)) (c4)\n\n    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (p4)\n    c5 = Conv2D(128, (3, 3), activation='relu', padding='same') (c5)\n\n    u6 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (c5)\n    u6 = concatenate([u6, c4])\n    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (u6)\n    c6 = Conv2D(64, (3, 3), activation='relu', padding='same') (c6)\n\n    u7 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same') (c6)\n    u7 = concatenate([u7, c3])\n    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (u7)\n    c7 = Conv2D(32, (3, 3), activation='relu', padding='same') (c7)\n\n    u8 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same') (c7)\n    u8 = concatenate([u8, c2])\n    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (u8)\n    c8 = Conv2D(16, (3, 3), activation='relu', padding='same') (c8)\n\n    u9 = Conv2DTranspose(8, (2, 2), strides=(2, 2), padding='same') (c8)\n    u9 = concatenate([u9, c1], axis=3)\n    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (u9)\n    c9 = Conv2D(8, (3, 3), activation='relu', padding='same') (c9)\n\n    outputs = Conv2D(1, (1, 1), activation='sigmoid') (c9)\n    model = Model(inputs=[inputs], outputs=[outputs])\n    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\n    return model\nsmall_unet().summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2e57e7902202404581c3c53c097a1db264e0f077",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "def model_crossover(model1, model2):\n    model=small_ae()\n    #for each layer\n    w1=model1.get_weights()\n    w2=model2.get_weights()\n    #print(type(w1))    \n    #print(len(w1))\n    wf=[]\n    for x in range(len(w1)):\n        if len(w1[x].shape)==4:\n     #       print(\"4\")\n            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])    \n            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])    \n            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2],w1[x].shape[3])*2            \n            z = np.where(xx>0.5, w1[x], w2[x])\n            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n            wf.append(z)\n        if len(w1[x].shape)==3:\n      #      print(\"3\")\n            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])    \n            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])    \n            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1],w1[x].shape[2])*2            \n            z = np.where(xx>0.5, w1[x], w2[x])\n            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n            wf.append(z)\n        if len(w1[x].shape)==2:\n       #     print(\"2\")\n            xx=np.random.rand(w1[x].shape[0],w1[x].shape[1])    \n            mutation_choise=np.random.rand(w1[x].shape[0],w1[x].shape[1])    \n            mutation_val=np.random.rand(w1[x].shape[0],w1[x].shape[1])*2            \n            z = np.where(xx>0.5, w1[x], w2[x])\n            z = np.where(mutation_choise>0.85,mutation_val*z,z)\n            wf.append(z)\n        if len(w1[x].shape)==1:\n        #    print(\"1\")\n            xx=np.random.rand(w1[x].shape[0])    \n            mutation_choise=np.random.rand(w1[x].shape[0])    \n            mutation_val=np.random.rand(w1[x].shape[0])*2            \n            z = np.where(xx>0.5, w1[x], w2[x])\n            z = np.where(mutation_choise>0.85,mutation_val*z,z)      \n   #         z=np.asscalar(z)\n            wf.append(z)\n            #print(z.shape)\n                                        \n    model.set_weights(wf)\n \n    return model\n      \n \ndef runtournament():\n    list_idx_on_tournament=[]\n    for x in range (tournament_sel):\n        list_idx_on_tournament.append(int(random.uniform(0, total_models-1)))\n        \n    best1=-999999999999999\n    best2=-999999999999999\n    best1_idx=-999999999999999\n    best2_idx=-999999999999999\n    for  x in range (tournament_sel):\n        if fitness[list_idx_on_tournament[x]]>best1:\n            best1=fitness[x]\n            best1_idx=x\n            \n    for  x in range (tournament_sel):\n        if fitness[list_idx_on_tournament[x]]>best2 and x!=best1_idx:\n            best2=fitness[x]\n            best2_idx=x        \n    return current_pool[list_idx_on_tournament[best1_idx]],current_pool[list_idx_on_tournament[best2_idx]]\n\ndef next_batch(num, data, labels):\n    '''\n    Return a total of `num` random samples and labels. \n    '''\n    idx = np.arange(0 , len(data))\n    np.random.shuffle(idx)\n    idx = idx[:num]\n    data_shuffle = [data[ i] for i in idx]\n    labels_shuffle = [labels[ i] for i in idx]\n\n    return np.asarray(data_shuffle), np.asarray(labels_shuffle)\ndef filter_image(img):\n    if img.sum() < 100:\n        return np.zeros(img.shape)\n    else:\n        return img\n\n## Scoring for last model\n#thresholds = np.linspace(0.3, 0.7, 31)\n\n\ndef parallel_scoring(i,x_train,y_train):    \n    \n    preds_valid=current_pool[x].predict(x_train)        \n    ious = fitnes_lossfunction(y_train.reshape((-1, img_size_target, img_size_target)), [filter_image(img) for img in preds_valid > 0.5 ])    \n    \n    return ious\n    \n    \ndef parallel_muttion(i):\n    model1,model2=runtournament()\n    model=model_crossover(model1,model2)\n    return model\n\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "collapsed": true,
        "_uuid": "e04865b70013b1199b3e39c4ceae1548327adfdd"
      },
      "cell_type": "code",
      "source": "current_pool = [] #actual networks saved\nfitness = [] #save value for each network\ntotal_models = 75 #we load 75 models in random state\ngenerations=50 #we perform and train GA over 50 generationand end\nbestfitness_index=0\ntournament_sel=5 #usefull for tournament selection method\n        \n    \n# Initialize all models with random weigth\nfor i in range(total_models):\n    # model\n    model=small_ae()\n    weights = model.get_weights()\n    weights = [np.random.permutation(w.flat).reshape(w.shape) for w in weights]\n    model.set_weights(weights)\n    #model.summary()\n    current_pool.append(model)\n    fitness.append(-100)    \n\n\n            \n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "48c1da3414a909a8dd6dd03ca653d03ee18502ac",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "\nprint(\"start generations\")\n\n\n#num_cores = multiprocessing.cpu_count()\n#inputs = range(total_models)\n\nfor i in range(generations):\n        print(\"Generation: \",i)\n        results=[]\n        #with concurrent.futures.ProcessPoolExecutor() as executor:\n            #for out1 in executor.map(parallel_scoring, range(0, total_models)):\n                #results.append(out1)\n        #xx_train,yy_train=next_batch(128,x_train,y_train)\n        for x in range(total_models):\n            results.append(parallel_scoring(x,x_train,y_train))                \n#        results = Parallel(n_jobs=num_cores)(delayed(parallel_scoring)(current_pool[i]) for i in inputs)\n        \n        \n        fitness=results    \n        bestfitness_index = np.argmax(np.array(fitness)) \n        print ('BEst Fitnes val {}- best fitnes index {}',fitness[bestfitness_index],bestfitness_index)\n        best_model=current_pool[bestfitness_index]#we save this model and save this as last\n        new_pool=[]        \n        for x in range(total_models-1):\n            model=parallel_muttion(i)\n            new_pool.append(model)        \n            #with concurrent.futures.ProcessPoolExecutor() as executor:\n            #for out1 in executor.map(parallel_muttion, range(0, total_models-1)):\n             #   new_pool.append(out1)                                    \n        current_pool=new_pool\n        current_pool.append(best_model)\n        \n            \n\nprint ('BEst Fitnes val {}- best fitnes index {}',fitness[bestfitness_index],bestfitness_index)\n            \n            \n        ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}